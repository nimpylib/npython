import ./lexer_utils
import deques
import sets
import strformat
import strutils
import typetraits
import tables
import parseutils

import token
import ../Utils/[utils, compat]

type
  # save source file for traceback info
  Source = ref object
    lines: seq[string]

  Mode* {.pure.} = enum
    Single
    File
    Eval

  Lexer* = ref object
    indentStack: seq[int] # Stack to track indentation levels
    lineNo: int
    tokenNodes*: seq[TokenNode] # might be consumed by parser
    fileName*: string

template indentLevel(lexer: Lexer): int = lexer.indentStack[^1]

var sourceFiles = initTable[string, Source]()

proc addSource*(filePath, content: string) = 
  if not sourceFiles.hasKey(filePath):
    sourceFiles[filePath] = new Source
  let s = sourceFiles[filePath]
  # s.lines.add content.split("\n")
  s.lines.addCompat content.split("\n")

proc getSource*(filePath: string, lineNo: int): string = 
  # lineNo starts from 1!
  sourceFiles[filePath].lines[lineNo-1]

proc `$`*(lexer: Lexer): string = 
  $lexer.tokenNodes


# used in parser.nim to construct non-terminators
proc newTokenNode*(token: Token, 
                   lineNo = -1, colNo = -1,
                   content = ""): TokenNode = 
  new result
  if token == Token.Name and content in reserveNameSet: 
    try:
      result = TokenNode(token: strTokenMap[content])
    except KeyError:
      unreachable
  else:
    case token
    of contentTokenSet:
      result = TokenNode(token: token, content: content)
    else:
      assert content == ""
      result = TokenNode(token: token)
  assert result.token != Token.NULLTOKEN
  if result.token.isTerminator:
    assert -1 < lineNo and -1 < colNo
    result.lineNo = lineNo
    result.colNo = colNo
  else:
    assert lineNo < 0 and colNo < 0


proc newLexer*(fileName: string): Lexer = 
  new result
  result.fileName = fileName
  result.indentStack = @[0] # Start with a single zero on the stack

# when we need a fresh start in interactive mode
proc clearTokens*(lexer: Lexer) = 
  # notnull checking issue for the compiler. gh-10651
  if lexer.tokenNodes.len != 0:
    lexer.tokenNodes.setLen 0

proc clearIndent*(lexer: Lexer) = 
  lexer.indentLevel = 0

proc add(lexer: Lexer, token: TokenNode) = 
  lexer.tokenNodes.add(token)

proc add(lexer: Lexer, token: Token, colNo:int) = 
  assert token.isTerminator
  lexer.add(newTokenNode(token, lexer.lineNo, colNo))

template delLast(s: seq) =
  discard s.pop()

# At the end of the file, generate DEDENT tokens for remaining stack levels
proc dedentAll*(lexer: Lexer) = 
  while lexer.indentStack.len > 1:
    lexer.indentStack.delLast()
    lexer.add(Token.Dedent, lexer.indentStack[^1])
    #dec lexer.indentLevel

# the function can probably be generated by a macro...
proc getNextToken(
  lexer: Lexer, 
  line: string, 
  idx: var int): TokenNode {. raises: [SyntaxError, InternalError] .} = 

  template raiseSyntaxError(msg: string) = 
    # fileName set elsewhere
    raiseSyntaxError(msg, "", lexer.lineNo, idx)

  template addToken(tokenName:untyped, msg:string) =
    var content: string
    let first = idx
    if not `parse tokenName`(line, content, start=idx):
      raiseSyntaxError(msg)
    idx.inc content.len
    result = newTokenNode(Token.tokenName, lexer.lineNo, first, content)

  template addSingleCharToken(tokenName) = 
    result = newTokenNode(Token.tokenName, lexer.lineNo, idx)
    inc idx

  template tailing(t: char): bool = 
    (idx < line.len - 1) and line[idx+1] == t

  template addSingleOrDoubleCharToken(tokenName1, tokenName2: untyped, c:char) = 
    if tailing(c):
      result = newTokenNode(Token.tokenName2, lexer.lineNo, idx)
      idx += 2
    else:
      addSingleCharToken(tokenName1)

  template newTokenNodeWithNo(Tk): TokenNode = 
      newTokenNode(Token.Tk, lexer.lineNo, idx)

  case line[idx]
  of 'a'..'z', 'A'..'Z', '_':
    addToken(Name, "Invalid identifier")
  of '0'..'9':
    addToken(Number, "Invalid number")
  of '"', '\'':
    let pairingChar = line[idx]
    
    if idx == line.len - 1:
      raiseSyntaxError("Invalid string syntax")
    let l = line.skipUntil(pairingChar, idx+1)
    if idx + l + 1 == line.len: # pairing `"` not found
      raiseSyntaxError("Invalid string syntax")
    else:
      result = newTokenNode(Token.String, lexer.lineNo, idx, line[idx+1..idx+l])
      idx += l + 2

  of '\n':
    result = newTokenNodeWithNo(Newline)
    idx += 1
  of '(':
    addSingleCharToken(Lpar)
  of ')':
    addSingleCharToken(Rpar)
  of '[':
    addSingleCharToken(Lsqb)
  of ']':
    addSingleCharToken(Rsqb)
  of ':':
    addSingleCharToken(Colon)
  of ',':
    addSingleCharToken(Comma)
  of ';':
    addSingleCharToken(Semi)
  of '+': 
    addSingleOrDoubleCharToken(Plus, PlusEqual, '=')
  of '-':
    if tailing('='):
      result = newTokenNodeWithNo(MinEqual)
      idx += 2
    elif tailing('>'):
      result = newTokenNodeWithNo(Rarrow)
      idx += 2
    else:
      addSingleCharToken(Minus)
  of '*':
    if tailing('*'):
      inc idx
      if tailing('='):
        result = newTokenNodeWithNo(DoubleStarEqual)
        idx += 2
      else:
        result = newTokenNodeWithNo(DoubleStar)
        inc idx
    else:
      addSingleCharToken(Star)
  of '/':
    if tailing('/'):
      inc idx
      if tailing('='):
        result = newTokenNodeWithNo(DoubleSlashEqual)
        idx += 2
      else:
        result = newTokenNodeWithNo(DoubleSlash)
        inc idx
    else:
      addSingleCharToken(Slash)
  of '|':
    addSingleOrDoubleCharToken(Vbar, VbarEqual, '=')
  of '&':
    addSingleOrDoubleCharToken(Amper, AmperEqual, '=')
  of '<': 
    if tailing('='):
      result = newTokenNodeWithNo(LessEqual)
      idx += 2
    elif tailing('<'):
      inc idx
      if tailing('='):
        result = newTokenNodeWithNo(LeftShiftEqual)
        idx += 2
      else:
        result = newTokenNodeWithNo(LeftShift)
        inc idx
    elif tailing('>'):
      raiseSyntaxError("<> in PEP401 not implemented")
    else:
      addSingleCharToken(Less)
  of '>':
    if tailing('='):
      result = newTokenNodeWithNo(GreaterEqual)
      idx += 2
    elif tailing('>'):
      inc idx
      if tailing('='):
        result = newTokenNodeWithNo(RightShiftEqual)
        idx += 2
      else:
        result = newTokenNodeWithNo(RightShift)
        inc idx
    else:
      addSingleCharToken(Greater)
  of '=': 
    addSingleOrDoubleCharToken(Equal, EqEqual, '=')
  of '.':
    if idx < line.len - 2 and line[idx+1] == '.' and line[idx+2] == '.':
      result = newTokenNodeWithNo(Ellipsis)
      idx += 3
    else:
      addSingleCharToken(Dot)
  of '%':
    addSingleOrDoubleCharToken(Percent, PercentEqual, '=')
  of '{':
    addSingleCharToken(Lbrace)
  of '}':
    addSingleCharToken(Rbrace)
  of '!':
    if tailing('='):
      inc idx
      addSingleCharToken(NotEqual)
    else:
      raiseSyntaxError("Single ! not allowed")
  of '~':
    addSingleCharToken(Tilde)
  of '^':
    addSingleOrDoubleCharToken(Circumflex, CircumflexEqual, '=')
  of '@':
    addSingleOrDoubleCharToken(At, AtEqual, '=')
  else: 
    raiseSyntaxError(fmt"Unknown character {line[idx]}")
  assert result != nil


proc lexOneLine(lexer: Lexer, line: string, mode: Mode) {.inline.} = 
  # Process one line at a time
  assert line.find("\n") == -1

  var idx = 0
  var indentLevel = 0

  # Calculate the indentation level based on spaces and tabs
  while idx < line.len:
    case line[idx]
    of ' ':
      indentLevel += 1
      inc(idx)
    of '\t':
      indentLevel += 8 # XXX: Assume a tab equals 8 spaces
      inc(idx)
    else:
      break

  if idx == line.len or line[idx] == '#': # Full of spaces or comment line
    return

  # Compare the calculated indentation level with the stack
  let currentIndent = lexer.indentLevel
  if indentLevel > currentIndent:
    lexer.indentStack.add(indentLevel)
    lexer.add(Token.Indent, idx)
  elif indentLevel < currentIndent:
    while lexer.indentLevel > indentLevel:
      lexer.indentStack.delLast()
      lexer.add(Token.Dedent, idx)
    if lexer.indentLevel != indentLevel:
      raiseSyntaxError "Indentation error", lexer.fileName, lexer.lineNo

  # Update the lexer's current indentation level
  lexer.indentLevel = indentLevel

  # Process the rest of the line for tokens
  while idx < line.len:
    case line[idx]
    of Whitespace - Newlines:
      inc idx
    of '#': # Comment line
      break
    else:
      lexer.add(getNextToken(lexer, line, idx))
  lexer.add(Token.NEWLINE, idx)

proc lexString*(lexer: Lexer, input: string, mode=Mode.File) = 
  assert mode != Mode.Eval # eval not tested

  # interactive mode and an empty line
  if mode == Mode.Single and input.len == 0:
    lexer.dedentAll
    lexer.add(Token.NEWLINE, 0)
    inc lexer.lineNo
    addSource(lexer.fileName, input)
    return

  for line in input.split("\n"):
    # lineNo starts from 1
    inc lexer.lineNo
    addSource(lexer.fileName, input)
    lexer.lexOneLine(line, mode)

  when defined(debug):
    echo lexer.tokenNodes

  case mode
  of Mode.File:
    lexer.dedentAll
    lexer.add(Token.Endmarker, 0)
  of Mode.Single:
    discard
  of Mode.Eval:
    lexer.add(Token.Endmarker, 0)

